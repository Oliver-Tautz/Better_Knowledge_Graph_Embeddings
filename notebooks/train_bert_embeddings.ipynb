{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c316ff-3679-4e37-bdc2-2742349ead62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid.nt', format='nt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30ad24-be71-4881-84cc-46166f67e333",
   "metadata": {},
   "source": [
    "#### Use objectives from T5 https://arxiv.org/abs/1910.10683\n",
    "\n",
    "Also look at Bert paper: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "First is 'Bert Style' masked language modeling. (MLM)\n",
    "\n",
    "* Corrupt 15% of input tokens. \n",
    "* 90% of the corrupted tokens are replaced with out-of-alphabet masking token\n",
    "* 10% of corrupted tokens are replaced with random tokens\n",
    "\n",
    "#### Other Sources\n",
    "\n",
    "* https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6 as example of Bert training\n",
    "* https://huggingface.co/blog/how-to-train\n",
    "* tiny bert https://arxiv.org/abs/2110.01518\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Bert: \"In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\"\n",
    "* Which tokenizer?! BPE useful? I think not. Look at https://huggingface.co/docs/transformers/main/en/tokenizer_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39096cd-5caf-43c7-8449-d1752c72275d",
   "metadata": {},
   "source": [
    "## Construct Dataset of 'Sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf83c9-9588-40dd-8e20-596021b80da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256f5636-e9cc-4209-9e1b-abb54d3abce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use each triple as a sentence ...\n",
    "dataset_most_simple = [' '.join(x) for x in g_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cfc1c8-1ce9-4dae-b32e-461101ee4407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://example.org/m/0l14md http://example.org/music/performance_role/track_performances./music/track_contribution/role http://example.org/m/03qlv7',\n",
       " 'http://example.org/m/011yg9 http://example.org/film/film/genre http://example.org/m/02l7c8',\n",
       " 'http://example.org/m/0565cz http://example.org/people/person/gender http://example.org/m/05zppz',\n",
       " 'http://example.org/m/033db3 http://example.org/people/person/profession http://example.org/m/02jknp',\n",
       " 'http://example.org/m/07m2y http://example.org/music/performance_role/track_performances./music/track_contribution/role http://example.org/m/02sgy',\n",
       " 'http://example.org/m/0cq7tx http://example.org/award/award_winning_work/awards_won./award/award_honor/award http://example.org/m/0gq9h',\n",
       " 'http://example.org/m/01p0vf http://example.org/music/artist/track_contributions./music/track_contribution/role http://example.org/m/03qjg',\n",
       " 'http://example.org/m/05qd_ http://example.org/award/award_nominee/award_nominations./award/award_nomination/award_nominee http://example.org/m/042kbj',\n",
       " 'http://example.org/m/01clyr http://example.org/music/record_label/artist http://example.org/m/0gbwp',\n",
       " 'http://example.org/m/02hnl http://example.org/music/performance_role/regular_performances./music/group_membership/group http://example.org/m/01518s']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_most_simple[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012589a7-a947-44b0-8441-24982659bdea",
   "metadata": {},
   "source": [
    "# Define and Train Tokenizer\n",
    "It is unclear which tokenizer works. I will start with a really simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4600f102-bdad-4038-9f7b-8fc2194e263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:51:28.961605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 14:51:28.961625: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, BertForTokenClassification\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import BertConfig, BertModel, AutoModel\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd0a5b5-e096-4fa6-86c3-ea0156a9d5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "special_tokens_map = tz.special_tokens_map_extended\n",
    "special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e8e41a-cbac-4a58-8520-a14b22123946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def random_mask(token,mask_token,vocab_size,mask_chance = 0.15, mask_token_chance=0.9):\n",
    "    mask_roll = torch.rand(())\n",
    "    if mask_roll < mask_chance:\n",
    "        mask_token_roll = torch.rand(())\n",
    "        if mask_token_roll < mask_token_chance:\n",
    "            return mask_token, 1\n",
    "        else:\n",
    "            return torch.randint(high=vocab_size,size=()), 2\n",
    "        \n",
    "    else:\n",
    "        return token, 0\n",
    "\n",
    "def mask_list_of_lists(l, mask_token,vocab_size,special_token_ids):\n",
    "\n",
    "    # get random mask for each token, but not for special tokens\n",
    "    return torch.tensor([[random_mask(y,mask_token,vocab_size) if y not in special_token_ids else y for y in x ] for x in l])\n",
    "\n",
    "def mask_list(l, mask_token,vocab_size,special_token_ids):\n",
    "\n",
    "    # get random mask for each token, but not for special tokens\n",
    "    return torch.tensor([random_mask(y,mask_token,vocab_size) if y not in special_token_ids else (y,0) for y in l])\n",
    "\n",
    "class dataseSimpleTriple(Dataset):\n",
    "    def __init__(self, triples,special_tokens_map,max_length=128):\n",
    "        \n",
    "        word_level_tokenizer = Tokenizer(WordLevel(unk_token=special_tokens_map['unk_token']))\n",
    "        word_level_trainer = WordLevelTrainer(special_tokens=list(special_tokens_map.values()))\n",
    "        # Pretokenizer. This is important and could lead to better/worse results!\n",
    "        word_level_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "        \n",
    "        word_level_tokenizer.train_from_iterator(dataset_most_simple,word_level_trainer)\n",
    "        \n",
    "        word_level_tokenizer.post_processor = BertProcessing(\n",
    "            (\"[SEP]\", word_level_tokenizer.token_to_id(\"[SEP]\")),\n",
    "            ('[CLS]', word_level_tokenizer.token_to_id('[CLS]')),\n",
    "        )\n",
    "        \n",
    "        mask_token = word_level_tokenizer.token_to_id(special_tokens_map['mask_token'])\n",
    "        word_level_tokenizer.enable_truncation(max_length=max_length)\n",
    "        self.labels = torch.tensor([x.ids for x in word_level_tokenizer.encode_batch(triples)])\n",
    "        \n",
    "\n",
    "        self.special_token_ids = [word_level_tokenizer.token_to_id(x) for x in special_tokens_map.values()]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.attention_masks = torch.stack([torch.ones(len(x)) for x in self.labels])\n",
    "        self.word_level_tokenizer = word_level_tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return mask_list(self.labels[i],self.word_level_tokenizer.token_to_id(special_tokens_map['mask_token']),self.word_level_tokenizer.get_vocab_size(),self.special_token_ids), self.attention_masks[i], self.labels[i]\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        return self.word_level_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b6494d-262d-4e1d-9072-147eeab95368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_simple = dataseSimpleTriple(dataset_most_simple,special_tokens_map)\n",
    "tz = dataset_simple.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6f69ab-b8d2-48c1-80db-7f0c301d515d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  1],\n",
       "        [ 4,  1],\n",
       "        [ 4,  1],\n",
       "        [10,  0],\n",
       "        [10,  0],\n",
       "        [10,  0],\n",
       "        [10,  0],\n",
       "        [10,  0],\n",
       "        [10,  0],\n",
       "        [10,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([random_mask(10,tz.token_to_id(special_tokens_map['mask_token']),tz.get_vocab_size()) for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746f764f-9522-4e56-9fa4-3e6583fe3483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3,   0],\n",
       "         [127,   0],\n",
       "         [ 21,   0],\n",
       "         [582,   0],\n",
       "         [  1,   0]]),\n",
       " tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([  3, 127,  21, 582,   1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_simple[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187eff82-054d-4586-81a7-9e38d766d551",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "675d7722-9345-460b-8b46-73468b8c7a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tiny_pretrained = AutoModel.from_pretrained('prajjwal1/bert-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0018f162-92b9-4365-b067-a0e9d50c77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tiny_config = tiny_pretrained.config\n",
    "tiny_config._name_or_path=\"otautz/tiny\"\n",
    "\n",
    "encoder_config = copy.copy(tiny_config)\n",
    "encoder_config.is_decoder = False\n",
    "encoder_config.add_cross_attention = False\n",
    "encoder_config.num_labels=tz.get_vocab_size()\n",
    "\n",
    "del tiny_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952ccc35-e47c-4ba0-a1c4-b5fb920efc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_encoder  = BertForTokenClassification(encoder_config)\n",
    "lossF = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d4ce85-8d45-4a7c-bb86-3963a3848217",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset_simple,batch_size=1000,shuffle=True,pin_memory=True)\n",
    "optimizer = torch.optim.Adam(tiny_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "796742dc-d633-464e-85f0-f6e8cd6b6a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                      | 0/1000 [01:21<?, ?it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_99465/1031614443.py\", line 11, in <module>\n",
      "    out = tiny_encoder.forward(batch_id, batch_mask)\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1757, in forward\n",
      "    logits = self.classifier(sequence_output)\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/olli/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1670, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1628, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.10/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
      "  File \"/usr/lib/python3.10/posixpath.py\", line 430, in _joinrealpath\n",
      "    st = os.lstat(newpath)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_99465/1031614443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiny_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1756\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "\n",
    "loss_metric = torchmetrics.aggregation.MeanMetric()\n",
    "history = defaultdict(list)\n",
    "\n",
    "for epochs in trange(10):\n",
    "    for inputs, batch_mask, batch_labels in dl:\n",
    "        optimizer.zero_grad()\n",
    "        batch_id = inputs[:,:,0]\n",
    "        \n",
    "        out = tiny_encoder.forward(batch_id, batch_mask)\n",
    "        logits = out.logits\n",
    "\n",
    "        # (batchsize, sequence_len, no_labels)\n",
    "        logits_shape = logits.shape\n",
    "\n",
    "        # (batchsize * sequence_len, no_labels)\n",
    "        logits_no_sequence =  logits.reshape(logits_shape[0]*logits_shape[1],logits_shape[2])\n",
    "\n",
    "        # (batchsize)\n",
    "        batch_labels_no_sequence = batch_labels.flatten()\n",
    "        \n",
    "        batch_mask = (inputs[:,:,1]>0).flatten()\n",
    "\n",
    "        loss= lossF(logits_no_sequence[batch_mask],batch_labels_no_sequence[batch_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_metric(loss)\n",
    "    \n",
    "    history['loss'].append(loss_metric.compute().item())\n",
    "    loss_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6c550-9fc7-466b-a554-0aac8a175d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a02e39-59b9-4652-a65b-a33c963a777d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std-env",
   "language": "python",
   "name": "std-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
