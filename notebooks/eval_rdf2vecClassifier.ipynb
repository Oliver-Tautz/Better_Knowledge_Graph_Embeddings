{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b868c6eb",
   "metadata": {},
   "source": [
    "## Download and check rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae41f5-8379-478a-80b4-8703c94916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f4da3",
   "metadata": {},
   "source": [
    "## Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6a4166-a9ac-41ca-8c70-df8812e17253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/otautz/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "wget: /home/otautz/miniconda3/lib/libuuid.so.1: no version information available (required by wget)\n",
      "/bin/bash: /home/otautz/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Archive:  fb15k-237_nt.zip\n"
     ]
    }
   ],
   "source": [
    "# download .nt dataset from my drive\n",
    "! wget -q -nc --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pBnn8bjI2VkVvBR33DnvpeyocfDhMCFA' -O fb15k-237_nt.zip\n",
    "\n",
    "! unzip -n fb15k-237_nt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9e650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def get_entities(graphs):\n",
    "    # get subjects and objects\n",
    "    entities = []\n",
    "    \n",
    "    for g in graphs:\n",
    "        entities = entities + list(g.subjects(unique=True)) + list(g.objects(unique=True))\n",
    "\n",
    "    # pythons stupid version of nub\n",
    "    entities = list(dict.fromkeys(entities))\n",
    "    return entities\n",
    "\n",
    "def get_all_corrupted_triples_fast(triple,entities,position = 'object'):\n",
    "    # not faster ...\n",
    "\n",
    "    s,p,o = triple\n",
    "\n",
    "    object_augmented = [(x,y,z) for  (x,y), z in itertools.product([triple[0:2]],entities)]\n",
    "    subject_augmented =[(x,y,z) for  x, (y,z) in itertools.product(entities,[triple[1:3]])]\n",
    "    \n",
    "    \n",
    "    return itertools.chain(object_augmented , subject_augmented)\n",
    "\n",
    "def get_all_corrupted_triples(triple,entities):\n",
    "    #too slow ....\n",
    "    \n",
    "    s,p,o = triple\n",
    "    subject_corrupted = [(s_corr,p,o) for s_corr in entities if s_corr != s]\n",
    "    object_corrupted = [(s,p,o_corr)   for o_corr in entities if o_corr != o]\n",
    "\n",
    "    return subject_corrupted + object_corrupted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def choose_many_multiple(arrs,n):\n",
    "    l = len(arrs[0])\n",
    "    for a in arrs:\n",
    "        assert len(a) == l, 'Arres not of same length ! :('\n",
    "        \n",
    "    \n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    \n",
    "    return [np.array(a)[ix] for a in arrs]\n",
    "    \n",
    "def choose_many(a,n):\n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    return np.array(a)[ix]\n",
    "    \n",
    "def choose(a):\n",
    "\n",
    "    L = len(a)\n",
    "\n",
    "    i = np.random.randint(0,L)\n",
    "\n",
    "    return a[i]\n",
    "\n",
    "def get_random_corrupted_triple(triple,entities, corrupt='object'):\n",
    "    \"\"\"\n",
    "    corrupt = one of 'subject', 'object', 'both'\n",
    "    \n",
    "    return corrupted triple with random entity\n",
    "    \"\"\"\n",
    "\n",
    "    s,p,o = triple\n",
    "    \n",
    "    # set up as the same\n",
    "    s_corr = s\n",
    "    o_corr = o\n",
    "    \n",
    "    if corrupt == 'subject':  \n",
    "        # corrupt only the subject\n",
    "        while s_corr == s:\n",
    "            s_corr = choose(entities)  \n",
    "    elif corrupt == 'object':\n",
    "        # corrupt only the object\n",
    "        while o_corr == o:\n",
    "            o_corr = choose(entities)  \n",
    "    elif corrupt == 'random':\n",
    "        # corrupt one or both randomly\n",
    "        ch = np.random.randint(3)\n",
    "        \n",
    "        if ch == 0:\n",
    "            while s_corr == s:\n",
    "                s_corr = choose(entities)  \n",
    "        if ch == 1 :\n",
    "            while o_corr == o:\n",
    "                o_corr = choose(entities)  \n",
    "        if ch == 2:\n",
    "            while s_corr == s or o_corr == o:\n",
    "                s_corr = choose(entities)  \n",
    "                o_corr = choose(entities) \n",
    "    else:\n",
    "        while s_corr == s or o_corr == o:\n",
    "            s_corr = choose(entities)  \n",
    "            o_corr = choose(entities) \n",
    "            \n",
    "    \n",
    "    return (s_corr,p,o_corr)\n",
    "    \n",
    "def merge_historires(history_list):\n",
    "    h = {}\n",
    "    for key in history_list[0].history.keys():\n",
    "        h[key] = [h.history[key][0] for h in histories]\n",
    "    return h    \n",
    "\n",
    "\n",
    "def clean_graph(graph,wv):\n",
    "    \"\"\"\n",
    "    clean graph such that all triples have word vectors present in wv\n",
    "    \n",
    "    \"\"\"\n",
    "    no_removed = 0 \n",
    "    for t in graph:\n",
    "        s,p,o = t\n",
    "        if not str(s) in wv.key_to_index.keys() or not str(p) in wv.key_to_index.keys() or not str(o) in wv.key_to_index.keys():\n",
    "            graph.remove(t)\n",
    "            no_removed+=1\n",
    "    return no_removed\n",
    "    \n",
    "    \n",
    "def get_vectors_fast(triples,entity_vec_mapping,vector_size=VECTOR_SIZE):\n",
    "    # ~20-30% faster\n",
    "    X = np.array(triples)\n",
    "    X = word_vectors[X.flatten()].reshape(len(triples),vector_size*3)\n",
    "    \n",
    "    return X    \n",
    "\n",
    "def get_vectors(triples,entity_vec_mapping,vector_size=200):\n",
    "    X = np.array(triples)\n",
    "    X = [(entity_vec_mapping(x[0]), entity_vec_mapping(x[1]),entity_vec_mapping(x[2])) for x in X]\n",
    "    X = [np.concatenate(x) for x in X]\n",
    "    X = np.vstack(X).astype(np.float64)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_1_1_dataset(graph, entities,entity_vec_mapping,corrupt='random'):\n",
    "    \n",
    "    original_triple_len = len(graph)\n",
    "    # get triples\n",
    "    X = list(graph)\n",
    "    no_t = len(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    corrupted_triples = [get_random_corrupted_triple(x,entities,corrupt=corrupt) for x in X]\n",
    "    X = X + corrupted_triples\n",
    "    \n",
    "    \n",
    "\n",
    "    # convert uris to strings\n",
    "    \n",
    "    X = get_vectors_fast(X,entity_vec_mapping)\n",
    "    \n",
    "    # stack them\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def test_sklearn_model(model,X,Y,x_test,y_test,subset=10000):\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    ix = np.random.choice(range(len(X)),size=subset)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    \n",
    "    X_scaled = scaler.transform(X[ix])\n",
    "    model.fit(X_scaled,Y[ix])\n",
    "\n",
    "    print(f'train_score ={model.score(scaler.transform(X),Y)}')    \n",
    "    print(f'test_score ={model.score(scaler.transform(x_test),y_test)}')\n",
    "\n",
    "def scale_and_predict(model,x):\n",
    "    x = preprocessing.StandardScaler().fit_transform(x)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97199e0-c550-49f1-a095-5ef70d0497d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word_vectors = Word2Vec.load('walks/model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187391ce-a626-47f8-8f9f-104f3f2a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0a6e85-44a1-469d-ae05-1e9b56425bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otautz/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pytorch model\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class ClassifierSimple(torch.nn.Module):\n",
    "    def __init__(self,input_dim=300,hidden_size=64):\n",
    "        super(ClassifierSimple, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                # flatten input if necessary\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(input_dim,hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size,1)\n",
    "        )\n",
    "        \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "                \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "        \n",
    "    \n",
    "    def forward(self,x):        \n",
    "        \n",
    "        return self.layers(x)\n",
    "    def predict(self,x):\n",
    "        x.to(self.device)\n",
    "        \n",
    "        return self.output_activation(self.layers(x))\n",
    "    def predict_numpy(self,x):\n",
    "        x = torch.tensor(x)\n",
    "        x.to(self.device)\n",
    "        return self.output_activation(self.layers(x)).detach().cpu().numpy()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0810bf13-53a4-4829-b1af-6137f2e2029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found trained model! Loading :)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "model = ClassifierSimple()    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if Path('rdf2vecClassfier.pth').is_file():\n",
    "    print('found trained model! Loading :)')\n",
    "    model.load_state_dict(torch.load('rdf2vecClassfier.pth'))\n",
    "    history = pd.read_csv('log.csv')\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print('model not found. Train it with ''train_rdf2vec_classifier.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95ab1b2-1076-4550-9e28-71659f588af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f117e933-8fc0-45b4-8ad0-32ab312e474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_concat(se,pe,oe):\n",
    "    \n",
    "    assert se.shape == pe.shape, \"Error! fast_concat with differing shapes\"\n",
    "    assert se.shape == oe.shape, \"Error! fast_concat with differing shapes\"\n",
    "    \n",
    "    \n",
    "    x = np.empty((se.shape[0],se.shape[1]*3),dtype=np.float32)\n",
    "    x[:,0:100] =se\n",
    "    x[:,100:200] = pe\n",
    "    x[:,200:] = oe\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaff9a5d-fb1f-4882-8d62-f77d2603e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_s_o_tuples(entities):\n",
    "    return [(s,o) for s in entities for o in entities] + [(o,s) for s in entities for o in entities]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea8294f-161a-4ec6-9335-46eeae2bee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_if_row_in_array(row,arr):\n",
    "    return np.any(np.sum(arr == row,axis=1) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e886ad23-0789-497f-9e0b-040423a16648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(triples,graphs):\n",
    "    # Too slow ... parallelize?! What else? Faster lookup? How?\n",
    "    graphs = [np.array(g) for g in graphs]\n",
    "\n",
    "    graphs = np.concatenate(graphs,axis=0)\n",
    "    \n",
    "    known_ix = []\n",
    "    \n",
    "    for i,tp in enumerate(triples):\n",
    "        if test_if_row_in_array(tp,graphs):\n",
    "            known_ix.append(i)\n",
    "    return known_ix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba56b73e-b86d-40b1-84d1-9e5779922b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bcfeb3c-d41d-4eee-848c-7af559960d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(scores,ix,mask=None):\n",
    "    if mask == None:    \n",
    "        optimistic_rank =(scores > scores[ix]).sum()+1\n",
    "        pessimistic_rank = (scores >= scores[ix]).sum()\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        optimistic_rank = ((scores > scores[ix]).index_fill(0,mask,False)).sum()+1\n",
    "        pessimistic_rank = ((scores >= scores[ix]).index_fill(0,mask,False)).sum()\n",
    "        \n",
    "    rank = (optimistic_rank+pessimistic_rank)*0.5\n",
    "        \n",
    "    return rank\n",
    "\n",
    "def parse_rdflib_to_torch(graph):\n",
    "    entities = get_entities([graph])\n",
    "    \n",
    "    entities=np.array(entities)\n",
    "    entity_vecs= torch.tensor(word_vectors[np.array(entities)])\n",
    "    entities = dict(zip(entities,range(len(entities))))\n",
    "    \n",
    "    predicates = np.array(list(set(graph.predicates())))\n",
    "    predicate_vecs = torch.tensor(word_vectors[predicates])\n",
    "    predicates = dict(zip(predicates,range(len(predicates))))\n",
    "    \n",
    "    edges = []\n",
    "    predicate_ix = []\n",
    "    for s,p,o in np.array(graph):\n",
    "        try:\n",
    "            edge = (entities[s],entities[o])\n",
    "            edges.append(edge)\n",
    "            predicate_ix.append(predicates[p])\n",
    "        except:\n",
    "            print(f\"Unknown entities encountered! ({s},{o})\")\n",
    "\n",
    "    edges = torch.tensor(edges)\n",
    "    predicate_ix = torch.tensor(predicate_ix)\n",
    "    \n",
    "    return edges, predicate_ix, entities, predicates,entity_vecs,predicate_vecs\n",
    "\n",
    "def get_comb_ix(edge,no_entities):\n",
    "    return edge[0]*no_entities+edge[1]\n",
    "\n",
    "def create_mask(length,ix,reverse=False):\n",
    "    if not reverse:\n",
    "        mask = torch.ones(length)\n",
    "        mask = mask.index_fill(0,ix,0)\n",
    "    else:\n",
    "        mask = torch.zeros(length)\n",
    "        mask = mask.index_fill(0,ix,1)\n",
    "    \n",
    "    \n",
    "    return mask\n",
    "\n",
    "def get_headmask(x,no_entities):\n",
    "    # ix from x to x+1\n",
    "    return torch.arange(x*no_entities,x*no_entities+no_entities)\n",
    "\n",
    "def get_tailmask(x,no_entities):\n",
    "    # ix x, x+1, x+2 ...\n",
    "    return torch.arange(x,no_entities**2,no_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04e2ea19-599e-430b-9b92-41712b5f5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timer import PerfTimer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from collections import defaultdict\n",
    "\n",
    "def eval_ranks(model,graph,filtered = True, batchsize=None, vecsize=100,force_cpu=False,bit16tensors = False,bench=False):\n",
    "# get data from graph\n",
    "    \n",
    "    \n",
    "    ### Setup ###\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if force_cpu:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        \n",
    "    if bit16tensors:\n",
    "        model = model.half().to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    perfTimer = PerfTimer()\n",
    "    \n",
    "    perfTimer.start()\n",
    "    \n",
    "    ### Preprocessing graph into torch arrays and ix\n",
    "    edges, predicate_ix, entity_ix_mapping, predicate_ix_mapping,entity_vecs,predicate_vecs = parse_rdflib_to_torch(graph)\n",
    "   \n",
    "    \n",
    "    entity_vecs = entity_vecs.to(device)\n",
    "    ix_predicate_mapping = {k:v for (v,k) in predicate_ix_mapping.items()}\n",
    "    ix_entity_mapping = {k:v for (v,k) in entity_ix_mapping.items()}\n",
    "    \n",
    "    no_entities  = len(entity_ix_mapping.keys())\n",
    "    \n",
    "    # could also try torch.cross and torch.combinations if needed\n",
    "    # all possible s_o_combinations\n",
    "    s_o_combs = torch.tensor(cartesian((range(no_entities),range(no_entities))))\n",
    "    # Allocate array only once\n",
    "    \n",
    "    if not batchsize:\n",
    "        # Full array in RAM\n",
    "        if bit16tensors:\n",
    "            to_score_embeddings = torch.empty((len(s_o_combs),vecsize*3)).half().to(device)\n",
    "        else:\n",
    "            to_score_embeddings = torch.empty((len(s_o_combs),vecsize*3)).to(device)\n",
    "               \n",
    "        to_score_embeddings[:,0:vecsize] =  entity_vecs[s_o_combs[:,0]]\n",
    "        to_score_embeddings[:,2*vecsize:]  = entity_vecs[s_o_combs[:,1]]          \n",
    "    else:\n",
    "        # Allocate Array of (Batchsize,dim)\n",
    "        if bit16tensors:\n",
    "            to_score_embeddings = torch.empty((batchsize,vecsize*3)).half().to(device)\n",
    "        else:\n",
    "            to_score_embeddings = torch.empty((batchsize,vecsize*3)).to(device)\n",
    "        \n",
    "\n",
    "        \n",
    "    head_ranks = []\n",
    "    tail_ranks = []\n",
    "    \n",
    "                   \n",
    "    \n",
    "    perfTimer.track('preprocessing')  \n",
    "    \n",
    "    for pred_ix in tqdm(ix_predicate_mapping.keys()):\n",
    "        # loop over all predicates\n",
    "        \n",
    "        current_edges = edges[predicate_ix == pred_ix]\n",
    "        number_of_real_edges = len(current_edges)\n",
    "        \n",
    "        perfTimer.track('subgraph')\n",
    "        \n",
    "\n",
    "        \n",
    "        if batchsize:\n",
    "            dl = DataLoader(s_o_combs, batch_size=batchsize, shuffle=False)\n",
    "            perfTimer.track('dl')\n",
    "            predicate_embedding = predicate_vecs[pred_ix]\n",
    "            \n",
    "            #use expand as memory of rows is shared. This may cause bugs ... investigate!\n",
    "            predicate_embedding = predicate_embedding.reshape(1,vecsize).expand(batchsize,vecsize)\n",
    "            perfTimer.track('expand_pediacte')\n",
    "            \n",
    "            scores = []\n",
    "\n",
    "            for batch in tqdm(dl):\n",
    "                #print(s_o_combs[:,0].type())\n",
    "                #print(batch[:,0].type())\n",
    "                if len(batch) != batchsize:\n",
    "                    \n",
    "                    to_score_embeddings[:,0:vecsize][0:len(batch)] = entity_vecs[batch[:,0]]\n",
    "                    to_score_embeddings[:,2*vecsize:][0:len(batch)]  = entity_vecs[batch[:,1]]  \n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "                    \n",
    "                    perfTimer.track('copy embeddings into array')\n",
    "\n",
    "                    batch_scores = model(to_score_embeddings)\n",
    "                    perfTimer.track('predict_batch')\n",
    "                    batch_scores = batch_scores[0:len(batch)]\n",
    "                    scores.append(batch_scores)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    to_score_embeddings[:,0:vecsize] =  entity_vecs[batch[:,0]]\n",
    "                    to_score_embeddings[:,2*vecsize:]  = entity_vecs[batch[:,1]]    \n",
    "                    to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "                    \n",
    "                    perfTimer.track('copy embeddings into array')\n",
    "\n",
    "\n",
    "                    batch_scores = model(to_score_embeddings)\n",
    "                    perfTimer.track('predict_batch')\n",
    "                    scores.append(batch_scores)\n",
    "                \n",
    "            scores = torch.vstack(scores).squeeze()\n",
    "            perfTimer.track('stack_all')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "           \n",
    "            \n",
    "            predicate_embedding = predicate_vecs[pred_ix]\n",
    "            #use expand as memory of rows is shared. This may cause bugs ... investigate!\n",
    "            predicate_embedding = predicate_embedding.reshape(1,100).expand(len(to_score_embeddings),100)\n",
    "            \n",
    "            perfTimer.track('collect_embeddings')\n",
    "            to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "            perfTimer.track('copy embeddings into array')\n",
    "\n",
    "\n",
    "            scores = model(to_score_embeddings).squeeze()\n",
    "            perfTimer.track('score_embeddings')\n",
    "            \n",
    "            return scores, edges, predicate_embedding,pred_ix , entity_vecs, predicate_ix\n",
    "            \n",
    "        #sorted_ix = scor\n",
    "\n",
    "            \n",
    "\n",
    "        head_edge_ix = None\n",
    "        tail_edge_ix = None\n",
    "\n",
    "        \n",
    "        for head in torch.unique(current_edges[:,0]):\n",
    "\n",
    "            head_edges = current_edges[current_edges[:,0] == head]\n",
    "            headmask = get_headmask(head,len(ix_entity_mapping.keys()))\n",
    "            \n",
    "            headscores = scores.index_select(0,headmask)\n",
    "         \n",
    "                        \n",
    "            if filtered:\n",
    "                head_edge_ix  = current_edges[:,1]\n",
    "            \n",
    "            \n",
    "            for edge in head_edges:\n",
    "                head_rank = compute_rank(headscores,edge[1],head_edge_ix)\n",
    "                head_ranks.append(head_rank)\n",
    "                \n",
    "            \n",
    "            \n",
    "        headscores = None\n",
    "        headmask = None\n",
    "        \n",
    "        for tail in torch.unique(current_edges[:,1]):\n",
    "            \n",
    "            tail_edges = current_edges[current_edges[:,1] == tail]\n",
    "            \n",
    "            tailmask = get_tailmask(tail,len(ix_entity_mapping.keys()))\n",
    "            tailscores = scores.index_select(0,tailmask)\n",
    "            \n",
    "            if filtered:\n",
    "                tail_edge_ix  = current_edges[:,0]\n",
    "            \n",
    "            for edge in tail_edges:\n",
    "                tail_rank = compute_rank(tailscores,edge[0],tail_edge_ix)\n",
    "                tail_ranks.append(tail_rank)\n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "                    \n",
    "        perfTimer.track('rank_embeddings')\n",
    "        if bench: \n",
    "            break\n",
    "    return torch.tensor(head_ranks),torch.tensor(tail_ranks), perfTimer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01609f08-ac88-4e78-a733-3a6a60dc5779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d7795d-6c3f-4b7a-ae0e-2db37220354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rank(head_ranks,tail_ranks):\n",
    "    return (torch.mean(head_ranks)+torch.mean(tail_ranks))/2\n",
    "\n",
    "def mean_reciprocal_rank(head_ranks,tail_ranks):\n",
    "    return (torch.mean(1/head_ranks)+torch.mean(1/tail_ranks))/2\n",
    "\n",
    "def hitsAt(head_ranks,tail_ranks,at):\n",
    "    return ((head_ranks <=at).sum() + (tail_ranks<=at).sum())/(len(head_ranks)+len(tail_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c23be91-0434-4bcb-a209-1f3a0ca8a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 270 triples from training set\n",
      "removed 35 triples from validation set\n",
      "removed 61 triples from test set\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "g_test = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid.nt', format='nt')\n",
    "g_test  = g_test.parse('FB15k-237/test.nt', format='nt')\n",
    "\n",
    "\n",
    "# clean graphs \n",
    "# number of triples removed should be low, a few hundred\n",
    "print(f\"removed {clean_graph(g_train,word_vectors)} triples from training set\")\n",
    "print(f\"removed {clean_graph(g_val,word_vectors)} triples from validation set\")\n",
    "print(f\"removed {clean_graph(g_test,word_vectors)} triples from test set\")\n",
    "\n",
    "entities = get_entities((g_train,g_val,g_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e69ef853-2962-422b-bcb5-bd6907242f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ddd4cc-df32-454e-bb83-d05d603bc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96c66bb1-8c6a-428c-b730-e629090ddd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/224 [00:07<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    scores, edges, predicate_embedding,pred_ix , entity_vecs, predicate_ix = eval_ranks(model,g_test,force_cpu=True,filtered=True,bit16tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af2978c-2da2-4385-9cc8-46eadca24416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timer import pickle_save\n",
    "pickle_save([scores, edges, predicate_embedding,pred_ix , entity_vecs, predicate_ix],'plot_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69874c5e-2be7-45b3-be0a-3abf0cd6d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -sh filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6dcba-40c6-4c16-a9c1-3ba95c7e06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested scoring on cpu\n",
    "torch.set_num_threads(32)\n",
    "with torch.no_grad():\n",
    "    head_ranks, tail_ranks,pt = eval_ranks(model,g_test,force_cpu=True,filtered=True,bit16tensors=False)\n",
    "    \n",
    "    print(f\"MR: {mean_rank(head_ranks, tail_ranks)}\")\n",
    "    print(f\"MRR: {mean_reciprocal_rank(head_ranks, tail_ranks)}\")\n",
    "    \n",
    "    print(f\"H@1: {hitsAt(head_ranks, tail_ranks,1)}\")\n",
    "    print(f\"H@3: {hitsAt(head_ranks, tail_ranks,3)}\")\n",
    "    print(f\"H@10: {hitsAt(head_ranks, tail_ranks,10)}\")\n",
    "    \n",
    "    \n",
    "    print(f\"MR_head: {torch.mean(head_ranks)}\")\n",
    "    print(f\"MRR_head: {torch.mean(1/head_ranks)}\")\n",
    "    \n",
    "    print(f\"H@1_head: {(head_ranks <= 1).sum()/len(tail_ranks)}\")\n",
    "    print(f\"H@3_head: {(head_ranks <= 3).sum()/len(tail_ranks)}\")\n",
    "    print(f\"H@10_head: {(head_ranks <= 10).sum()/len(tail_ranks)}\")\n",
    "    \n",
    "    \n",
    "    print(f\"MR_tail: {torch.mean(tail_ranks)}\")\n",
    "    print(f\"MRR_tail: {torch.mean(1/tail_ranks)}\")\n",
    "    \n",
    "    print(f\"H@1_tail: {(tail_ranks <= 1).sum()/len(tail_ranks)}\")\n",
    "    print(f\"H@3_tail: {(tail_ranks <= 3).sum()/len(tail_ranks)}\")\n",
    "    print(f\"H@10_tail: {(tail_ranks <= 10).sum()/len(tail_ranks)}\")\n",
    "    \n",
    "    \n",
    "gc.collect()\n",
    "pt.stats_sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aab68c-6928-4c92-98a6-e0d716cf90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('here!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
