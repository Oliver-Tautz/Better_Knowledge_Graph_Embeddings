{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b868c6eb",
   "metadata": {},
   "source": [
    "## Download and check rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae41f5-8379-478a-80b4-8703c94916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 0\n",
    "VECTOR_SIZE=100\n",
    "test_classic_ml = False\n",
    "CLASSIFIER_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f252b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/otautz/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "wget: /home/otautz/miniconda3/lib/libuuid.so.1: no version information available (required by wget)\n",
      "File ‘jrdf2vec-1.3-SNAPSHOT.jar’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download jrdf2vec\n",
    "! wget -nc https://raw.githubusercontent.com/dwslab/jRDF2Vec/jars/jars/jrdf2vec-1.3-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108c9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check jrdf2vec installation. Should print \"=> Everything is installed. You are good to go!\" somewhere\n",
    "if training:\n",
    "    ! java -jar jrdf2vec-1.3-SNAPSHOT.jar -checkInstallation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f4da3",
   "metadata": {},
   "source": [
    "## Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6a4166-a9ac-41ca-8c70-df8812e17253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/otautz/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "wget: /home/otautz/miniconda3/lib/libuuid.so.1: no version information available (required by wget)\n",
      "/bin/bash: /home/otautz/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Archive:  fb15k-237_nt.zip\n"
     ]
    }
   ],
   "source": [
    "# download .nt dataset from my drive\n",
    "! wget -q -nc --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pBnn8bjI2VkVvBR33DnvpeyocfDhMCFA' -O fb15k-237_nt.zip\n",
    "\n",
    "! unzip -n fb15k-237_nt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09bcf",
   "metadata": {},
   "source": [
    "## Generate walks and train rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a781daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate walks and train rdf2vec on train set with default parameters\n",
    "if training:\n",
    "    ! rm -rf walks\n",
    "    ! java -jar jrdf2vec-1.3-SNAPSHOT.jar -walkDirectory walks -graph FB15k-237/train.nt -threads 10 -dimension 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9e650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(graphs):\n",
    "    # get subjects and objects\n",
    "    entities = []\n",
    "    \n",
    "    for g in graphs:\n",
    "        entities = entities + list(g.subjects(unique=True)) + list(g.objects(unique=True))\n",
    "\n",
    "    # pythons stupid version of nub\n",
    "    entities = list(dict.fromkeys(entities))\n",
    "    return entities\n",
    "\n",
    "def get_all_corrupted_triples_fast(triple,entities,position = 'object'):\n",
    "    # not faster ...\n",
    "\n",
    "    s,p,o = triple\n",
    "\n",
    "    object_augmented = [(x,y,z) for  (x,y), z in itertools.product([triple[0:2]],entities)]\n",
    "    subject_augmented =[(x,y,z) for  x, (y,z) in itertools.product(entities,[triple[1:3]])]\n",
    "    \n",
    "    \n",
    "    return itertools.chain(object_augmented , subject_augmented)\n",
    "\n",
    "def get_all_corrupted_triples(triple,entities):\n",
    "    #too slow ....\n",
    "    \n",
    "    s,p,o = triple\n",
    "    subject_corrupted = [(s_corr,p,o) for s_corr in entities if s_corr != s]\n",
    "    object_corrupted = [(s,p,o_corr)   for o_corr in entities if o_corr != o]\n",
    "\n",
    "    return subject_corrupted + object_corrupted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def choose_many_multiple(arrs,n):\n",
    "    l = len(arrs[0])\n",
    "    for a in arrs:\n",
    "        assert len(a) == l, 'Arres not of same length ! :('\n",
    "        \n",
    "    \n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    \n",
    "    return [np.array(a)[ix] for a in arrs]\n",
    "    \n",
    "def choose_many(a,n):\n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    return np.array(a)[ix]\n",
    "    \n",
    "def choose(a):\n",
    "\n",
    "    L = len(a)\n",
    "\n",
    "    i = np.random.randint(0,L)\n",
    "\n",
    "    return a[i]\n",
    "\n",
    "def get_random_corrupted_triple(triple,entities, corrupt='object'):\n",
    "    \"\"\"\n",
    "    corrupt = one of 'subject', 'object', 'both'\n",
    "    \n",
    "    return corrupted triple with random entity\n",
    "    \"\"\"\n",
    "\n",
    "    s,p,o = triple\n",
    "    \n",
    "    # set up as the same\n",
    "    s_corr = s\n",
    "    o_corr = o\n",
    "    \n",
    "    if corrupt == 'subject':  \n",
    "        # corrupt only the subject\n",
    "        while s_corr == s:\n",
    "            s_corr = choose(entities)  \n",
    "    elif corrupt == 'object':\n",
    "        # corrupt only the object\n",
    "        while o_corr == o:\n",
    "            o_corr = choose(entities)  \n",
    "    elif corrupt == 'random':\n",
    "        # corrupt one or both randomly\n",
    "        ch = np.random.randint(3)\n",
    "        \n",
    "        if ch == 0:\n",
    "            while s_corr == s:\n",
    "                s_corr = choose(entities)  \n",
    "        if ch == 1 :\n",
    "            while o_corr == o:\n",
    "                o_corr = choose(entities)  \n",
    "        if ch == 2:\n",
    "            while s_corr == s or o_corr == o:\n",
    "                s_corr = choose(entities)  \n",
    "                o_corr = choose(entities) \n",
    "    else:\n",
    "        while s_corr == s or o_corr == o:\n",
    "            s_corr = choose(entities)  \n",
    "            o_corr = choose(entities) \n",
    "            \n",
    "    \n",
    "    return (s_corr,p,o_corr)\n",
    "    \n",
    "def merge_historires(history_list):\n",
    "    h = {}\n",
    "    for key in history_list[0].history.keys():\n",
    "        h[key] = [h.history[key][0] for h in histories]\n",
    "    return h    \n",
    "\n",
    "\n",
    "def clean_graph(graph,wv):\n",
    "    \"\"\"\n",
    "    clean graph such that all triples have word vectors present in wv\n",
    "    \n",
    "    \"\"\"\n",
    "    no_removed = 0 \n",
    "    for t in graph:\n",
    "        s,p,o = t\n",
    "        if not str(s) in wv.key_to_index.keys() or not str(p) in wv.key_to_index.keys() or not str(o) in wv.key_to_index.keys():\n",
    "            graph.remove(t)\n",
    "            no_removed+=1\n",
    "    return no_removed\n",
    "    \n",
    "    \n",
    "def get_vectors_fast(triples,entity_vec_mapping,vector_size=VECTOR_SIZE):\n",
    "    # ~20-30% faster\n",
    "    X = np.array(triples)\n",
    "    X = word_vectors[X.flatten()].reshape(len(triples),vector_size*3)\n",
    "    \n",
    "    return X    \n",
    "\n",
    "def get_vectors(triples,entity_vec_mapping,vector_size=200):\n",
    "    X = np.array(triples)\n",
    "    X = [(entity_vec_mapping(x[0]), entity_vec_mapping(x[1]),entity_vec_mapping(x[2])) for x in X]\n",
    "    X = [np.concatenate(x) for x in X]\n",
    "    X = np.vstack(X).astype(np.float64)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_1_1_dataset(graph, entities,entity_vec_mapping,corrupt='random'):\n",
    "    \n",
    "    original_triple_len = len(graph)\n",
    "    # get triples\n",
    "    X = list(graph)\n",
    "    no_t = len(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    corrupted_triples = [get_random_corrupted_triple(x,entities,corrupt=corrupt) for x in X]\n",
    "    X = X + corrupted_triples\n",
    "    \n",
    "    \n",
    "\n",
    "    # convert uris to strings\n",
    "    \n",
    "    X = get_vectors_fast(X,entity_vec_mapping)\n",
    "    \n",
    "    # stack them\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def test_sklearn_model(model,X,Y,x_test,y_test,subset=10000):\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    ix = np.random.choice(range(len(X)),size=subset)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    \n",
    "    X_scaled = scaler.transform(X[ix])\n",
    "    model.fit(X_scaled,Y[ix])\n",
    "\n",
    "    print(f'train_score ={model.score(scaler.transform(X),Y)}')    \n",
    "    print(f'test_score ={model.score(scaler.transform(x_test),y_test)}')\n",
    "\n",
    "def scale_and_predict(model,x):\n",
    "    x = preprocessing.StandardScaler().fit_transform(x)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06e7fd",
   "metadata": {},
   "source": [
    "## Parse Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e64f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "#import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "g_test = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid.nt', format='nt')\n",
    "g_test  = g_test.parse('FB15k-237/test.nt', format='nt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f1346",
   "metadata": {},
   "source": [
    "## Plot Graph (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c538a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://stackoverflow.com/questions/39274216/visualize-an-rdflib-graph-in-python\n",
    "# takes way too long ... use subgraph?!\n",
    "import matplotlib.pyplot as plt\n",
    "plot = False\n",
    "if plot:\n",
    "    G = rdflib_to_networkx_multidigraph(g_train[0:100])\n",
    "\n",
    "\n",
    "    pos = nx.spring_layout(G, scale=2)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'r')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    nx.draw(G, with_labels=True)\n",
    "\n",
    "    #if not in interactive mode for \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97199e0-c550-49f1-a095-5ef70d0497d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word_vectors = Word2Vec.load('walks/model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187391ce-a626-47f8-8f9f-104f3f2a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a40410a-9f37-4b6a-97b7-343741815a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 270 triples from training set\n",
      "removed 35 triples from validation set\n",
      "removed 61 triples from test set\n"
     ]
    }
   ],
   "source": [
    "# clean graphs \n",
    "# number of triples removed should be low, a few hundred\n",
    "print(f\"removed {clean_graph(g_train,word_vectors)} triples from training set\")\n",
    "print(f\"removed {clean_graph(g_val,word_vectors)} triples from validation set\")\n",
    "print(f\"removed {clean_graph(g_test,word_vectors)} triples from test set\")\n",
    "\n",
    "entities = get_entities((g_train,g_val,g_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f4d9fe-b9b3-4fc9-9d56-782da104f938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14385"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc90e67-dfb0-4187-84ce-096fd5208b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training datapoints = 543690\n",
      "validation datapoints = 35000\n",
      "test datapoints = 40810\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x])\n",
    "x_val , y_val= get_1_1_dataset(g_val,entities,lambda x : word_vectors[x])\n",
    "x_test , y_test= get_1_1_dataset(g_test,entities,lambda x : word_vectors[x])\n",
    "\n",
    "print(f\"training datapoints = {len(X)}\")\n",
    "print(f\"validation datapoints = {len(x_val)}\")\n",
    "print(f\"test datapoints = {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01240b70-aebb-4a99-8be5-3b9be3f3068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some simple baselines\\\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded6c5bf-f5f3-4d17-a371-964bdeb391b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_classic_ml:\n",
    "    # 0.6 is pretty bad\n",
    "    LR = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "    test_sklearn_model(LR,X,Y,x_test,y_test,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4befa0-03f2-456d-aa7c-3cacf0d59c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_classic_ml:\n",
    "    # this works pretty well out of the box\n",
    "    randomforest = sklearn.ensemble.RandomForestClassifier()\n",
    "    test_sklearn_model(randomforest,X,Y,x_test,y_test,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c93d06f2-d2d5-4029-821e-3b14aa2ac64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1_1_dataset(graph, entities,entity_vec_mapping,corrupt='random'):\n",
    "    \n",
    "    original_triple_len = len(graph)\n",
    "    # get triples\n",
    "    X = list(graph)\n",
    "    no_t = len(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    corrupted_triples = [get_random_corrupted_triple(x,entities,corrupt=corrupt) for x in X]\n",
    "    X = X + corrupted_triples\n",
    "    \n",
    "    \n",
    "\n",
    "    # convert uris to strings\n",
    "    \n",
    "    X = get_vectors_fast(X,entity_vec_mapping)\n",
    "    \n",
    "    # stack them\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc530634-4104-4a1f-ad74-4fc216a51337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otautz/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def get_random_corrupted_triple_embedded(triple,entities, corrupt='object',vector_size=VECTOR_SIZE):\n",
    "    \"\"\"\n",
    "    corrupt = one of 'subject', 'object', 'both'\n",
    "    \n",
    "    return corrupted triple with random entity\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    s = triple[0:VECTOR_SIZE]\n",
    "    p = triple[VECTOR_SIZE:VECTOR_SIZE*2]\n",
    "    o = triple[VECTOR_SIZE*2:]\n",
    "    \n",
    "    # set up as the same\n",
    "    s_corr = s[:]\n",
    "    o_corr = o[:]\n",
    "\n",
    "    if corrupt == 'subject':  \n",
    "   \n",
    "        # corrupt only the subject\n",
    "        while (s_corr == s).all():\n",
    "            s_corr = choose(entities)  \n",
    "    elif corrupt == 'object':\n",
    "        # corrupt only the object\n",
    "        while (o_corr == o).all():\n",
    "            o_corr = choose(entities)  \n",
    "    elif corrupt == 'random':\n",
    "        # corrupt one or both randomly\n",
    "        ch = np.random.randint(3)\n",
    "        \n",
    "        if ch == 0:\n",
    "            while (s_corr== s).all():\n",
    "                s_corr = choose(entities)  \n",
    "        if ch == 1 :\n",
    "            while (o_corr == o).all():\n",
    "                o_corr = choose(entities)  \n",
    "        if ch == 2:\n",
    "\n",
    "            while (s_corr == s).all() or (o_corr == o).all():\n",
    "                s_corr = choose(entities)  \n",
    "                o_corr = choose(entities) \n",
    "    else:\n",
    "\n",
    "        while (s_corr == s).all() or (o_corr == o).all():\n",
    "            s_corr = choose(entities)  \n",
    "            o_corr = choose(entities) \n",
    "            \n",
    "    \n",
    "    return np.concatenate((s_corr,p,o_corr),axis=0)\n",
    "\n",
    "def get_1_1_dataset_embedded(graph, entities,corrupt='random', vector_size=VECTOR_SIZE):\n",
    "    \"\"\"\n",
    "    graph: numpy array of shape (samples,3*\n",
    "    \"\"\"\n",
    "  \n",
    "    if len(graph.shape) == 1:\n",
    "        graph = np.expand_dims(graph,0)\n",
    "    #print(graph.shape)\n",
    "    no_t = len(graph)\n",
    "    corrupted_triples = [get_random_corrupted_triple_embedded(x,entities,corrupt=corrupt,vector_size=vector_size) for x in graph]\n",
    "    X = np.concatenate((graph,corrupted_triples),axis=0)\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y\n",
    "import torch\n",
    "class Dataset11(torch.utils.data.Dataset):\n",
    "    def __init__(self,graph,vec_mapping,entities,corrupt='random',vector_size=VECTOR_SIZE):\n",
    "        \"\"\"\n",
    "        graph: graph to train on\n",
    "        vec_mapping: function that returns vectos from URIs\n",
    "        entities: iterable of all entities to build fake triples\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.entities = vec_mapping(np.array(entities))        \n",
    "        self.graph = get_vectors_fast(graph,vec_mapping)\n",
    "        self.len = len(graph)\n",
    "        self.vec_mapping = vec_mapping\n",
    "        self.corrupt = corrupt\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return self.len\n",
    "    def __getitem__(self,ix):\n",
    "        return get_1_1_dataset_embedded(self.graph[ix],self.entities,self.corrupt,self.vector_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7383975f-30c5-4468-8726-e29ed51403ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange,tqdm\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def fit_1_1(model,graph,word_vec_mapping,batch_size,entities,metrics=None,epochs=50,optimizer=None,lossF= torch.nn.BCEWithLogitsLoss(),graph_eval=None):\n",
    "    \"\"\"\n",
    "    metrics: dictionary of {name:torchmetric} to track for training and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # move metrics to device \n",
    "    metrics = {name:metric.to(device) for (name,metric) in metrics.items()}\n",
    "    \n",
    "    # move model to device\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if not optimizer:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    loss_metric = torchmetrics.aggregation.MeanMetric().to(device)\n",
    "    \n",
    "    dataset = Dataset11(graph,word_vec_mapping,entities,'random')\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    if graph_eval:\n",
    "        dataset_eval = Dataset11(graph_eval,word_vec_mapping,entities,'random')\n",
    "        dataloader_eval = DataLoader(dataset_eval,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    for ep in trange(epochs):\n",
    "        # train \n",
    "        for X,Y in dataloader:\n",
    "            model.train()\n",
    "            \n",
    "            # shape is (bs,2,3*VECTORSIZE) because of dataset implementation\n",
    "            # so I need to flatten the arrays\n",
    "            X = torch.flatten(X,0,1)\n",
    "            Y = torch.flatten(Y)\n",
    "\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device).double()\n",
    "            \n",
    "            predictions = model(X).squeeze()\n",
    "            loss = lossF(predictions,Y)\n",
    "            loss_metric(loss)          \n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for metric in metrics.values():\n",
    "                metric(predictions,Y.long())\n",
    "            \n",
    "            \n",
    "        for name, metric in metrics.items():\n",
    "            history[name].append(metric.compute())\n",
    "            metric.reset()\n",
    "        history['loss'].append(loss_metric.compute() )   \n",
    "        loss_metric.reset() \n",
    "            \n",
    "        # eval\n",
    "        if graph_eval:\n",
    "            with torch.no_grad():\n",
    "                for X,Y in dataloader:\n",
    "                    model.eval()\n",
    "\n",
    "                    # shape is (bs,2,3*VECTORSIZE) because of dataset implementation\n",
    "                    # so I need to flatten the arrays\n",
    "                    X = torch.flatten(X,0,1)\n",
    "                    Y = torch.flatten(Y)\n",
    "\n",
    "                    X = X.to(device)\n",
    "                    Y= Y.to(device).double()\n",
    "\n",
    "\n",
    "                    predictions = model(X).squeeze()\n",
    "                    loss = lossF(predictions,Y)\n",
    "                    loss_metric(loss)\n",
    "\n",
    "\n",
    "                    for metric in metrics.values():\n",
    "                        metric(predictions,Y.long())\n",
    "\n",
    "                for name, metric in metrics.items():\n",
    "                    history[name+'_val'].append(metric.compute())\n",
    "                    metric.reset()\n",
    "                history['loss_val'].append(loss_metric.compute())\n",
    "                loss_metric.reset() \n",
    "    # tensor - > float conversion\n",
    "    history = {k: [x.item() for x in v] for (k,v) in history.items()}\n",
    "    return model, history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e6409a7-096f-46d1-9612-7b305d87e1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b0a6e85-44a1-469d-ae05-1e9b56425bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class ClassifierSimple(torch.nn.Module):\n",
    "    def __init__(self,input_dim=300,hidden_size=64):\n",
    "        super(ClassifierSimple, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                # flatten input if necessary\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(input_dim,hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size,1)\n",
    "        )\n",
    "        \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "                \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "        \n",
    "    \n",
    "    def forward(self,x):        \n",
    "        \n",
    "        return self.layers(x)\n",
    "    def predict(self,x):\n",
    "        x.to(self.device)\n",
    "        \n",
    "        return self.output_activation(self.layers(x))\n",
    "    def predict_numpy(self,x):\n",
    "        x = torch.tensor(x)\n",
    "        x.to(self.device)\n",
    "        return self.output_activation(self.layers(x)).detach().cpu().numpy()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810bf13-53a4-4829-b1af-6137f2e2029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 16/300 [04:45<1:24:12, 17.79s/it]"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "model = ClassifierSimple()    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if Path('rdf2vecClassfier.pth').is_file():\n",
    "    print('found trained model! Loading :)')\n",
    "    model.load_state_dict(torch.load('rdf2vecClassfier.pth'))\n",
    "    history = pd.read_csv('log.csv')\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model = model.to(device)\n",
    "    model,history = fit_1_1(model,g_train,lambda x: word_vectors[x],3000,entities,metrics = {'acc' :torchmetrics.classification.Accuracy()},graph_eval=g_val,epochs=300)\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(),'rdf2vecClassfier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1b879-0606-41d6-8d49-ac514a4be3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history)[['acc','acc_val']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfa919-8f2e-4c67-a6bd-aa0b4e6f8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history)[['loss','loss_val']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ab1b2-1076-4550-9e28-71659f588af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_link_pred(score_f,graph,entity_vec_mapping,entities,vector_size = 100, max_triples=100, plot = False):\n",
    "    print('start_evaluating ...')\n",
    "    \n",
    "    entities = np.array(entities)\n",
    "    \n",
    "    stats = {'generate_time' : [],\n",
    "        'convert_time:': [],\n",
    "        'score_time': [],\n",
    "        'sort_time': [],\n",
    "        'total_time': []}\n",
    "\n",
    "    ranks = []\n",
    "    \n",
    "    \n",
    "    # slightly complicated defaultdict trick. Uninitialised 2dim matrix. \n",
    "    # Only store what was seen. Return None for other values\n",
    "\n",
    "    embeddings_scores = defaultdict(lambda: defaultdict(lambda :None))\n",
    "\n",
    "    \n",
    "    graph = np.array(graph)\n",
    "    \n",
    "    if max_triples:\n",
    "        graph = choose_many(graph,max_triples)\n",
    "    \n",
    "    \n",
    "    for i,tp in enumerate(tqdm(np.array(graph))):\n",
    "                                   \n",
    "        s,p,o = tp\n",
    "        start_timer = time.perf_counter()\n",
    "        # construct corrupted triples\n",
    "\n",
    "        triples_to_test = np.array([tp]+get_all_corrupted_triples(tp,entities))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        generate_timer = time.perf_counter()\n",
    "        \n",
    "        if p in embeddings_scores.keys():\n",
    "            \n",
    "            # init\n",
    "            lookup = embeddings_scores[p]\n",
    "            lookup_scores = []\n",
    "            ix_to_score = []\n",
    "            \n",
    "            # find scores and triple not scored\n",
    "            for j,ttt in enumerate(triples_to_test):\n",
    "                s,p,o = ttt\n",
    "                \n",
    "                if (s,o) in lookup.keys():\n",
    "                    l = lookup[(s,o)]\n",
    "                    lookup_scores.append((l,j))\n",
    "                else:\n",
    "\n",
    "                    ix_to_score.append(j)\n",
    "            #print(pd.DataFrame(triples_to_test[ix_to_score]))\n",
    "            pd.DataFrame(triples_to_test[ix_to_score], columns = ['s','p','o']).to_feather(f'debug/not_found_{i}.feather')\n",
    "            pd.DataFrame(lookup.keys(), columns = ['s','o']).to_feather(f'debug/keys_{i}.feather')\n",
    "            pd.DataFrame([[len(lookup_scores),len(triples_to_test),len(lookup_scores)/len(triples_to_test)]], columns = ['n_looked_up','n','%']).to_feather(f'debug/stats_{i}.feather')\n",
    "            \n",
    "            print(f\"looked up {len(lookup_scores)} of {len(triples_to_test)} triples\")\n",
    "            # score unscored triples\n",
    "            triples_to_score = triples_to_test[ix_to_score]\n",
    "            if len(triples_to_score) > 0:\n",
    "                vectors_to_score = get_vectors_fast(triples_to_score,entity_vec_mapping,vector_size=vector_size)\n",
    "                convert_timer = time.perf_counter()\n",
    "                scores = score_f(vectors_to_score).numpy().squeeze()\n",
    "\n",
    "                # add scores to lookup\n",
    "                for ttt, score in zip(triples_to_score,scores):\n",
    "                    s, _ ,o = tp\n",
    "                    embeddings_scores[p][(s,o)] = score\n",
    "            else:\n",
    "                scores =[]\n",
    "                \n",
    "            score_timer = time.perf_counter()\n",
    "            \n",
    "            # combine scores\n",
    "\n",
    "            \n",
    "            scores =  lookup_scores + list(zip(scores,ix_to_score))\n",
    " \n",
    "            scores = np.array(scores)\n",
    "\n",
    "            # sort by score\n",
    "            scores = scores[np.argsort(scores[:,0])]\n",
    "            \n",
    "            # get rank\n",
    "            # look at col 1 (ix) and find original triple ix.\n",
    "            \n",
    "            rank = len(scores) - np.where(scores[:,1] == 0)[0][0]\n",
    "            sort_timer = time.perf_counter()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "        else:\n",
    "            vectors_to_test = get_vectors_fast(triples_to_test,entity_vec_mapping,vector_size=vector_size)\n",
    "            convert_timer = time.perf_counter()\n",
    "\n",
    "                # score them\n",
    "            scores = (score_f(vectors_to_test)).numpy().squeeze()\n",
    "            \n",
    "            for tp, score in zip(triples_to_test,scores):\n",
    "                    s,p,o = tp\n",
    "                    embeddings_scores[p][(s,o)] = score                        \n",
    "\n",
    "            if plot:\n",
    "                fig = plot_embeddings(vectors_to_test,scores)\n",
    "                fig.show()\n",
    "\n",
    "            score_timer = time.perf_counter()\n",
    "            # sort them \n",
    "            sort_ix = np.argsort(scores ,axis=0)\n",
    "            rank =  len(sort_ix) - np.where(sort_ix == 0)[0][0]\n",
    "         \n",
    "        \n",
    "            sort_timer = time.perf_counter()\n",
    "\n",
    "        \n",
    "\n",
    "        stats['generate_time'].append(generate_timer - start_timer)\n",
    "        stats['convert_time:'].append(convert_timer -  generate_timer)\n",
    "        stats['score_time'].append(score_timer-convert_timer)\n",
    "        stats['sort_time'].append(sort_timer-score_timer)\n",
    "        stats['total_time'].append(sort_timer-start_timer)\n",
    "        ranks.append(rank)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(rank),'\\n',{key: np.mean(value) for key,value in list(stats.items())[-100:]})\n",
    "            pass\n",
    "            \n",
    "        \n",
    "    stats =  {key: np.mean(value) for key,value in stats.items()}\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "    \n",
    "    stats['MR'] = np.mean(ranks)\n",
    "    stats['MRR'] = np.mean(1/(ranks))\n",
    "    stats['HITS1'] = np.count_nonzero(ranks <= 1)\n",
    "    stats['HITS3'] = np.count_nonzero(ranks <= 3)\n",
    "    stats['HITS10'] = np.count_nonzero(ranks <= 10)\n",
    "        \n",
    "    return  ranks, stats, embeddings_scores\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff9a5d-fb1f-4882-8d62-f77d2603e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_s_o_tuples(entities):\n",
    "    return [(s,o) for s in entities for o in entities] + [(o,s) for s in entities for o in entities]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8294f-161a-4ec6-9335-46eeae2bee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_if_row_in_array(row,arr):\n",
    "    return np.any(np.sum(arr == row,axis=1) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886ad23-0789-497f-9e0b-040423a16648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(triples,graphs):\n",
    "    # Too slow ... parallelize?! What else? Faster lookup? How?\n",
    "    graphs = [np.array(g) for g in graphs]\n",
    "\n",
    "    graphs = np.concatenate(graphs,axis=0)\n",
    "    \n",
    "    known_ix = []\n",
    "    \n",
    "    for i,tp in enumerate(triples):\n",
    "        if test_if_row_in_array(tp,graphs):\n",
    "            known_ix.append(i)\n",
    "    return known_ix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56b73e-b86d-40b1-84d1-9e5779922b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97b25b-2378-44a5-ba8a-40d13a74a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.utils.extmath import cartesian\n",
    "def evaluate_link_pred_fast(score_f,graph,entity_vec_mapping,entities,vector_size = 100, max_triples=100, plot = False, filter_by=None,verbose = True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    stats = {'preprocessing_time' : -1,\n",
    "        'embeddings_time': [],\n",
    "        'rank_time': [],\n",
    "        'find_rank_time':[],\n",
    "        'ranks':[]\n",
    "        }\n",
    "    \n",
    "    start_timer = time.perf_counter()\n",
    "    predicates = np.array(list(set(graph.predicates())))\n",
    "    \n",
    "    graph = np.array(graph)\n",
    "    \n",
    "    \n",
    "    print(f\"evaluate LP on graph with {len(graph)} triples, {len(entities)} entities and {len(predicates)} predicates!\")\n",
    "    \n",
    "    print(f\"Starting preprocessing\")\n",
    "    embeddings_scores = defaultdict(lambda: defaultdict(lambda :None))\n",
    "    \n",
    "    \n",
    "    \n",
    "    s_o_combinations = cartesian((entities,entities))   \n",
    "    # Why sort them?!\n",
    "    subjects = s_o_combinations[:,0] # sorted(s_o_combinations[:,0])\n",
    "    subject_embeddings = entity_vec_mapping(subjects)\n",
    "    \n",
    "    objects = s_o_combinations[:,1]  #sorted(s_o_combinations[:,1])\n",
    "    object_embeddings = entity_vec_mapping(objects)\n",
    "    \n",
    "    no_triples_per_predicate=len(subjects)\n",
    "    \n",
    "    \n",
    "    \n",
    "    preprocessing_timer = time.perf_counter()\n",
    "    \n",
    "    stats['preprocessing_time'] = preprocessing_timer - start_timer\n",
    "    \n",
    "    print(f\"Finished preprocessing\")\n",
    "    \n",
    "    del s_o_combinations\n",
    "    \n",
    "    \n",
    "    for p in tqdm(predicates):\n",
    "        predicate_start_timer = time.perf_counter()\n",
    "        \n",
    "        \n",
    "             \n",
    "        predicate_embedding = entity_vec_mapping([p])\n",
    "        predicate_column = np.repeat(predicate_embedding,no_triples_per_predicate).reshape(no_triples_per_predicate,vector_size)\n",
    "        \n",
    "        #return subject_embeddings,predicate_column,object_embeddings\n",
    "\n",
    "        triple_embeddings = np.concatenate([subject_embeddings,predicate_column,object_embeddings],axis=1)\n",
    "        \n",
    "        \n",
    "        predicate_embeddings_timer = time.perf_counter()\n",
    "        \n",
    "        del predicate_column\n",
    "\n",
    "        \n",
    "\n",
    "        scores = np.squeeze(score_f(triple_embeddings))\n",
    "        \n",
    "        del triple_embeddings\n",
    "        \n",
    "        \n",
    "        sorted_ix = np.flip(np.argsort(scores))       \n",
    "        scored_triples = np.stack([subjects,np.repeat(p,len(objects)),objects]).T   \n",
    "        scored_triples = scored_triples[sorted_ix]\n",
    "        \n",
    "        \n",
    "        predicate_rank_timer = time.perf_counter()\n",
    "        \n",
    "        ranks = []\n",
    "    \n",
    "\n",
    "        predicate_subgraph = graph[graph[:,1] == p]\n",
    "    \n",
    "        for triple in predicate_subgraph:\n",
    "            try:\n",
    "                rank = np.where(np.sum(scored_triples ==triple,axis=1) == 3)[0][0]\n",
    "                print(rank)\n",
    "                ranks.append(rank)\n",
    "            except:\n",
    "                print(triple)\n",
    "                print(np.where(np.sum(scored_triples ==triple,axis=1)))\n",
    "                print('unknown entity or relation!')\n",
    "                \n",
    "        \n",
    "        \n",
    "        predicate_find_rank = time.perf_counter()\n",
    "        \n",
    "            \n",
    "        stats['embeddings_time'].append(predicate_embeddings_timer-predicate_start_timer)\n",
    "        stats['rank_time'].append(predicate_rank_timer - predicate_embeddings_timer)\n",
    "        stats['find_rank_time'].append(predicate_find_rank - predicate_rank_timer)\n",
    "        stats['ranks'].extend(ranks)\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    return  stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ef830-ee02-4b97-93bd-9e6fe2ab7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "g_test = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid.nt', format='nt')\n",
    "g_test  = g_test.parse('FB15k-237/test.nt', format='nt')\n",
    "\n",
    "\n",
    "# clean graphs \n",
    "# number of triples removed should be low, a few hundred\n",
    "print(f\"removed {clean_graph(g_train,word_vectors)} triples from training set\")\n",
    "print(f\"removed {clean_graph(g_val,word_vectors)} triples from validation set\")\n",
    "print(f\"removed {clean_graph(g_test,word_vectors)} triples from test set\")\n",
    "\n",
    "entities = get_entities((g_train,g_val,g_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89654c3d-6b28-4459-b4eb-94ba7fa0c57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10f66b-59b9-4826-8e28-9d8922a38c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    stats = evaluate_link_pred_fast(lambda x : model.predict_numpy(x), g_test, lambda x : word_vectors[x], entities, max_triples=None)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f74b-8307-438b-936f-1f674e708afb",
   "metadata": {},
   "source": [
    "# Plot the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b1fb3-d088-4559-b050-5ac24734fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='both')\n",
    "#x_val , y_val= get_1_1_dataset(g_val,entities,lambda x : word_vectors[x])\n",
    "#x_test , y_test= get_1_1_dataset(g_test,entities,lambda x : word_vectors[x])\n",
    "\n",
    "x,y = X, Y\n",
    "#for x,y in [(X,Y),(x_val,y_val),(x_test,y_test)]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf0479-2467-40a5-b702-b8f960761751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "reduction_model = sklearn.manifold.TSNE(learning_rate='auto',init='pca').fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fde8b-19bf-42cd-ae51-3fa25b72032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(x,y,reduction_model=sklearn.manifold.TSNE(learning_rate='auto',init='pca').fit_transform,number_of_examples=10000):\n",
    "\"\"\"\n",
    "x = multi dim array (SAMPLES,EMBEDDING_DIM)\n",
    "y = one-dim array (SAMPLES,)\n",
    "\"\"\"\n",
    "    x,y = choose_many_multiple([x,y],number_of_examples)\n",
    "    x_reduced=reduction_model(x)\n",
    "\n",
    "    plot_data = np.concatenate((x_reduced,np.expand_dims(y,1)),1)\n",
    "\n",
    "    df = pd.DataFrame(plot_data,columns=list(range(x_tsne.shape[1]))+['label'])\n",
    "\n",
    "\n",
    "    fig = px.scatter(df, x=0, y=1, color=\"label\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9d692-da51-49cf-aae2-ae5a1f9ce590",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e24f8-7a75-4214-a9f5-1f59b720ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='both')\n",
    "\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275b55e-96cf-42f8-8ad9-c44d55b22bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='subject')\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387298af-5a0e-4dcc-9b56-2cae808552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='object')\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055a362",
   "metadata": {},
   "source": [
    "## Get walks into ram :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
