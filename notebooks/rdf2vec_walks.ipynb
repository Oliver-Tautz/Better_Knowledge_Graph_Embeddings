{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b868c6eb",
   "metadata": {},
   "source": [
    "## Download and check rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae41f5-8379-478a-80b4-8703c94916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 0\n",
    "VECTOR_SIZE=100\n",
    "test_classic_ml = False\n",
    "CLASSIFIER_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f252b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘jrdf2vec-1.3-SNAPSHOT.jar’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download jrdf2vec\n",
    "! wget -nc https://raw.githubusercontent.com/dwslab/jRDF2Vec/jars/jars/jrdf2vec-1.3-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108c9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check jrdf2vec installation. Should print \"=> Everything is installed. You are good to go!\" somewhere\n",
    "if training:\n",
    "    ! java -jar jrdf2vec-1.3-SNAPSHOT.jar -checkInstallation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f4da3",
   "metadata": {},
   "source": [
    "## Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6a4166-a9ac-41ca-8c70-df8812e17253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fb15k-237_nt.zip\n",
      "  inflating: FB15k-237/train.nt      \n",
      "  inflating: FB15k-237/test.nt       \n",
      "  inflating: FB15k-237/valid.nt      \n"
     ]
    }
   ],
   "source": [
    "# download .nt dataset from my drive\n",
    "! wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pBnn8bjI2VkVvBR33DnvpeyocfDhMCFA' -O fb15k-237_nt.zip\n",
    "\n",
    "! unzip -o fb15k-237_nt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09bcf",
   "metadata": {},
   "source": [
    "## Generate walks and train rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a781daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate walks and train rdf2vec on train set with default parameters\n",
    "if training:\n",
    "    ! rm -rf walks\n",
    "    ! java -jar jrdf2vec-1.3-SNAPSHOT.jar -walkDirectory walks -graph FB15k-237/train.nt -threads 10 -dimension 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9e650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(graphs):\n",
    "    # get subjects and objects\n",
    "    entities = []\n",
    "    \n",
    "    for g in graphs:\n",
    "        entities = entities + list(g.subjects(unique=True)) + list(g.objects(unique=True))\n",
    "\n",
    "    # pythons stupid version of nub\n",
    "    entities = list(dict.fromkeys(entities))\n",
    "    return entities\n",
    "\n",
    "def get_all_corrupted_triples_fast(triple,entities,position = 'object'):\n",
    "    # not faster ...\n",
    "\n",
    "    s,p,o = triple\n",
    "\n",
    "    object_augmented = [(x,y,z) for  (x,y), z in itertools.product([triple[0:2]],entities)]\n",
    "    subject_augmented =[(x,y,z) for  x, (y,z) in itertools.product(entities,[triple[1:3]])]\n",
    "    \n",
    "    \n",
    "    return itertools.chain(object_augmented , subject_augmented)\n",
    "\n",
    "def get_all_corrupted_triples(triple,entities):\n",
    "    #too slow ....\n",
    "    \n",
    "    s,p,o = triple\n",
    "    subject_corrupted = [(s_corr,p,o) for s_corr in entities if s_corr != s]\n",
    "    object_corrupted = [(s,p,o_corr)   for o_corr in entities if o_corr != o]\n",
    "\n",
    "    return subject_corrupted + object_corrupted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def choose_many_multiple(arrs,n):\n",
    "    l = len(arrs[0])\n",
    "    for a in arrs:\n",
    "        assert len(a) == l, 'Arres not of same length ! :('\n",
    "        \n",
    "    \n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    \n",
    "    return [np.array(a)[ix] for a in arrs]\n",
    "    \n",
    "def choose_many(a,n):\n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    return np.array(a)[ix]\n",
    "    \n",
    "def choose(a):\n",
    "\n",
    "    L = len(a)\n",
    "\n",
    "    i = np.random.randint(0,L)\n",
    "\n",
    "    return a[i]\n",
    "\n",
    "def get_random_corrupted_triple(triple,entities, corrupt='object'):\n",
    "    \"\"\"\n",
    "    corrupt = one of 'subject', 'object', 'both'\n",
    "    \n",
    "    return corrupted triple with random entity\n",
    "    \"\"\"\n",
    "\n",
    "    s,p,o = triple\n",
    "    \n",
    "    # set up as the same\n",
    "    s_corr = s\n",
    "    o_corr = o\n",
    "    \n",
    "    if corrupt == 'subject':  \n",
    "        # corrupt only the subject\n",
    "        while s_corr == s:\n",
    "            s_corr = choose(entities)  \n",
    "    elif corrupt == 'object':\n",
    "        # corrupt only the object\n",
    "        while o_corr == o:\n",
    "            o_corr = choose(entities)  \n",
    "    elif corrupt == 'random':\n",
    "        # corrupt one or both randomly\n",
    "        ch = np.random.randint(3)\n",
    "        \n",
    "        if ch == 0:\n",
    "            while s_corr == s:\n",
    "                s_corr = choose(entities)  \n",
    "        if ch == 1 :\n",
    "            while o_corr == o:\n",
    "                o_corr = choose(entities)  \n",
    "        if ch == 2:\n",
    "            while s_corr == s or o_corr == o:\n",
    "                s_corr = choose(entities)  \n",
    "                o_corr = choose(entities) \n",
    "    else:\n",
    "        while s_corr == s or o_corr == o:\n",
    "            s_corr = choose(entities)  \n",
    "            o_corr = choose(entities) \n",
    "            \n",
    "    \n",
    "    return (s_corr,p,o_corr)\n",
    "    \n",
    "def merge_historires(history_list):\n",
    "    h = {}\n",
    "    for key in history_list[0].history.keys():\n",
    "        h[key] = [h.history[key][0] for h in histories]\n",
    "    return h    \n",
    "\n",
    "\n",
    "def clean_graph(graph,wv):\n",
    "    \"\"\"\n",
    "    clean graph such that all triples have word vectors present in wv\n",
    "    \n",
    "    \"\"\"\n",
    "    no_removed = 0 \n",
    "    for t in graph:\n",
    "        s,p,o = t\n",
    "        if not str(s) in wv.key_to_index.keys() or not str(p) in wv.key_to_index.keys() or not str(o) in wv.key_to_index.keys():\n",
    "            graph.remove(t)\n",
    "            no_removed+=1\n",
    "    return no_removed\n",
    "    \n",
    "    \n",
    "def get_vectors_fast(triples,entity_vec_mapping,vector_size=VECTOR_SIZE):\n",
    "    # ~20-30% faster\n",
    "    X = np.array(triples)\n",
    "    X = word_vectors[X.flatten()].reshape(len(triples),vector_size*3)\n",
    "    \n",
    "    return X    \n",
    "\n",
    "def get_vectors(triples,entity_vec_mapping,vector_size=200):\n",
    "    X = np.array(triples)\n",
    "    X = [(entity_vec_mapping(x[0]), entity_vec_mapping(x[1]),entity_vec_mapping(x[2])) for x in X]\n",
    "    X = [np.concatenate(x) for x in X]\n",
    "    X = np.vstack(X).astype(np.float64)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_1_1_dataset(graph, entities,entity_vec_mapping,corrupt='random'):\n",
    "    \n",
    "    original_triple_len = len(graph)\n",
    "    # get triples\n",
    "    X = list(graph)\n",
    "    no_t = len(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    corrupted_triples = [get_random_corrupted_triple(x,entities,corrupt=corrupt) for x in X]\n",
    "    X = X + corrupted_triples\n",
    "    \n",
    "    \n",
    "\n",
    "    # convert uris to strings\n",
    "    \n",
    "    X = get_vectors_fast(X,entity_vec_mapping)\n",
    "    \n",
    "    # stack them\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def test_sklearn_model(model,X,Y,x_test,y_test,subset=10000):\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    ix = np.random.choice(range(len(X)),size=subset)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    \n",
    "    X_scaled = scaler.transform(X[ix])\n",
    "    model.fit(X_scaled,Y[ix])\n",
    "\n",
    "    print(f'train_score ={model.score(scaler.transform(X),Y)}')    \n",
    "    print(f'test_score ={model.score(scaler.transform(x_test),y_test)}')\n",
    "\n",
    "def scale_and_predict(model,x):\n",
    "    x = preprocessing.StandardScaler().fit_transform(x)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06e7fd",
   "metadata": {},
   "source": [
    "## Parse Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e64f1c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61729/911896176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mg_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FB15k-237/train.nt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mg_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mg_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FB15k-237/valid.nt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mg_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mg_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FB15k-237/test.nt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/rdflib/graph.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, publicID, format, location, file, data, **args)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;31m# TODO FIXME: Parser.parse should have **kwargs argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcould_not_guess_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/rdflib/plugins/parsers/ntriples.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(cls, source, sink, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW3CNTriplesParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNTGraphSink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/rdflib/plugins/parsers/ntriples.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, f, bnode_context)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnode_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnode_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParseError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid line: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/rdflib/plugins/parsers/ntriples.py\u001b[0m in \u001b[0;36mparseline\u001b[0;34m(self, bnode_context)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mobject_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnode_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stdpy/std_env/lib/python3.10/site-packages/rdflib/plugins/parsers/ntriples.py\u001b[0m in \u001b[0;36meat\u001b[0;34m(self, pattern)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# @@ Why can't we get the original pattern?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# print(dir(pattern))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "g_test = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid.nt', format='nt')\n",
    "g_test  = g_test.parse('FB15k-237/test.nt', format='nt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f1346",
   "metadata": {},
   "source": [
    "## Plot Graph (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c538a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://stackoverflow.com/questions/39274216/visualize-an-rdflib-graph-in-python\n",
    "# takes way too long ... use subgraph?!\n",
    "import matplotlib.pyplot as plt\n",
    "plot = False\n",
    "if plot:\n",
    "    G = rdflib_to_networkx_multidigraph(g_train[0:100])\n",
    "\n",
    "\n",
    "    pos = nx.spring_layout(G, scale=2)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'r')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    nx.draw(G, with_labels=True)\n",
    "\n",
    "    #if not in interactive mode for \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97199e0-c550-49f1-a095-5ef70d0497d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word_vectors = Word2Vec.load('walks/model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750374a-c8e0-4a52-83b0-ed8803299b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986903e-ed1a-4fe4-aaf0-072ca86e8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['http://example.org/award/award_nominee/award_nominations./award/award_nomination/award_nominee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c053c87-3446-4737-ae55-f530f9ea12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = np.array(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907c9b0-b737-495e-a875-0a5701be97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('word_vectors[entities]', number=1000,globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c1d86-3c5d-4cc9-b18f-3d3926af077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('[word_vectors.get_vector(x) for x in entities]', number=1000,globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187391ce-a626-47f8-8f9f-104f3f2a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \n",
    "    return (word_vectors.get_vector(x) for x in iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40410a-9f37-4b6a-97b7-343741815a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean graphs \n",
    "# number of triples removed should be low, a few hundred\n",
    "print(f\"removed {clean_graph(g_train,word_vectors)} triples from training set\")\n",
    "print(f\"removed {clean_graph(g_val,word_vectors)} triples from validation set\")\n",
    "print(f\"removed {clean_graph(g_test,word_vectors)} triples from test set\")\n",
    "\n",
    "entities = get_entities((g_train,g_val,g_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc90e67-dfb0-4187-84ce-096fd5208b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x])\n",
    "x_val , y_val= get_1_1_dataset(g_val,entities,lambda x : word_vectors[x])\n",
    "x_test , y_test= get_1_1_dataset(g_test,entities,lambda x : word_vectors[x])\n",
    "\n",
    "print(f\"training datapoints = {len(X)}\")\n",
    "print(f\"validation datapoints = {len(x_val)}\")\n",
    "print(f\"test datapoints = {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01240b70-aebb-4a99-8be5-3b9be3f3068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some simple baselines\\\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6c5bf-f5f3-4d17-a371-964bdeb391b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_classic_ml:\n",
    "    # 0.6 is pretty bad\n",
    "    LR = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "    test_sklearn_model(LR,X,Y,x_test,y_test,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4befa0-03f2-456d-aa7c-3cacf0d59c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_classic_ml:\n",
    "    # this works pretty well out of the box\n",
    "    randomforest = sklearn.ensemble.RandomForestClassifier()\n",
    "    test_sklearn_model(randomforest,X,Y,x_test,y_test,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#### Neural Network with a single hidden layer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tqdm import tqdm, trange\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(.3))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "            # Loss function to minimize\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=True,\n",
    "    label_smoothing=0.0,\n",
    "    axis=-1,\n",
    "    reduction=\"auto\",\n",
    "    name=\"binary_crossentropy\",\n",
    ")\n",
    ",\n",
    "            # List of metrics to monitor\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "modelname = Path(f'./rdf2vec_classifier/trained_for={CLASSIFIER_EPOCHS}/rdf2vec_classifier')\n",
    "\n",
    "try:\n",
    "    model.load_weights(modelname) \n",
    "    h = pd.read_csv(modelname.parent / 'training_log.csv')\n",
    "    print(\"trained weights found! not training again :)\")\n",
    "except:\n",
    "\n",
    "\n",
    "    histories = []\n",
    "\n",
    "    for epochs in trange(CLASSIFIER_EPOCHS):\n",
    "        X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x])\n",
    "        histories.append(model.fit(X, Y, batch_size=3000, epochs=1, validation_data=(x_val,y_val),))\n",
    "\n",
    "\n",
    "    model.save_weights(modelname) \n",
    "    h = merge_historires(histories)\n",
    "    pd.DataFrame(h).to_csv(modelname.parent / 'training_log.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a4eef-4ec6-48e7-a0ea-303023afbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(h)[['loss','val_loss']].plot()\n",
    "pd.DataFrame(h)[['accuracy','val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53f0a4-197b-463c-8cc1-dd193d502c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalu = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print(evalu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fcacf-a404-4b12-a891-34f0926a932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in g_train:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fda014-2a38-4a55-9ed4-f7f91d1998ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ab1b2-1076-4550-9e28-71659f588af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "def evaluate_link_pred(score_f,graph,entity_vec_mapping,entities,vector_size = 100, max_triples=100, plot = False):\n",
    "    print('start_evaluating ...')\n",
    "    \n",
    "    entities = np.array(entities)\n",
    "    \n",
    "    stats = {'generate_time' : [],\n",
    "        'convert_time:': [],\n",
    "        'score_time': [],\n",
    "        'sort_time': [],\n",
    "        'total_time': []}\n",
    "\n",
    "    ranks = []\n",
    "    \n",
    "    \n",
    "    # slightly complicated defaultdict trick. Uninitialised 2dim matrix. \n",
    "    # Only store what was seen. Return None for other values\n",
    "\n",
    "    embeddings_scores = defaultdict(lambda: defaultdict(lambda :None))\n",
    "\n",
    "    \n",
    "    graph = np.array(graph)\n",
    "    \n",
    "    if max_triples:\n",
    "        graph = choose_many(graph,max_triples)\n",
    "    \n",
    "    \n",
    "    for i,tp in enumerate(tqdm(np.array(graph))):\n",
    "                                   \n",
    "        s,p,o = tp\n",
    "        start_timer = time.perf_counter()\n",
    "        # construct corrupted triples\n",
    "\n",
    "        triples_to_test = np.array([tp]+get_all_corrupted_triples(tp,entities))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        generate_timer = time.perf_counter()\n",
    "        \n",
    "        if p in embeddings_scores.keys():\n",
    "            \n",
    "            # init\n",
    "            lookup = embeddings_scores[p]\n",
    "            lookup_scores = []\n",
    "            ix_to_score = []\n",
    "            \n",
    "            # find scores and triple not scored\n",
    "            for j,ttt in enumerate(triples_to_test):\n",
    "                s,p,o = ttt\n",
    "                \n",
    "                if (s,o) in lookup.keys():\n",
    "                    l = lookup[(s,o)]\n",
    "                    lookup_scores.append((l,j))\n",
    "                else:\n",
    "\n",
    "                    ix_to_score.append(j)\n",
    "            #print(pd.DataFrame(triples_to_test[ix_to_score]))\n",
    "            pd.DataFrame(triples_to_test[ix_to_score], columns = ['s','p','o']).to_feather(f'debug/not_found_{i}.feather')\n",
    "            pd.DataFrame(lookup.keys(), columns = ['s','o']).to_feather(f'debug/keys_{i}.feather')\n",
    "            pd.DataFrame([[len(lookup_scores),len(triples_to_test),len(lookup_scores)/len(triples_to_test)]], columns = ['n_looked_up','n','%']).to_feather(f'debug/stats_{i}.feather')\n",
    "            \n",
    "            print(f\"looked up {len(lookup_scores)} of {len(triples_to_test)} triples\")\n",
    "            # score unscored triples\n",
    "            triples_to_score = triples_to_test[ix_to_score]\n",
    "            if len(triples_to_score) > 0:\n",
    "                vectors_to_score = get_vectors_fast(triples_to_score,entity_vec_mapping,vector_size=vector_size)\n",
    "                convert_timer = time.perf_counter()\n",
    "                scores = score_f(vectors_to_score).numpy().squeeze()\n",
    "\n",
    "                # add scores to lookup\n",
    "                for ttt, score in zip(triples_to_score,scores):\n",
    "                    s, _ ,o = tp\n",
    "                    embeddings_scores[p][(s,o)] = score\n",
    "            else:\n",
    "                scores =[]\n",
    "                \n",
    "            score_timer = time.perf_counter()\n",
    "            \n",
    "            # combine scores\n",
    "\n",
    "            \n",
    "            scores =  lookup_scores + list(zip(scores,ix_to_score))\n",
    " \n",
    "            scores = np.array(scores)\n",
    "\n",
    "            # sort by score\n",
    "            scores = scores[np.argsort(scores[:,0])]\n",
    "            \n",
    "            # get rank\n",
    "            # look at col 1 (ix) and find original triple ix.\n",
    "            \n",
    "            rank = len(scores) - np.where(scores[:,1] == 0)[0][0]\n",
    "            sort_timer = time.perf_counter()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "        else:\n",
    "            vectors_to_test = get_vectors_fast(triples_to_test,entity_vec_mapping,vector_size=vector_size)\n",
    "            convert_timer = time.perf_counter()\n",
    "\n",
    "                # score them\n",
    "            scores = (score_f(vectors_to_test)).numpy().squeeze()\n",
    "            \n",
    "            for tp, score in zip(triples_to_test,scores):\n",
    "                    s,p,o = tp\n",
    "                    embeddings_scores[p][(s,o)] = score                        \n",
    "\n",
    "            if plot:\n",
    "                fig = plot_embeddings(vectors_to_test,scores)\n",
    "                fig.show()\n",
    "\n",
    "            score_timer = time.perf_counter()\n",
    "            # sort them \n",
    "            sort_ix = np.argsort(scores ,axis=0)\n",
    "            rank =  len(sort_ix) - np.where(sort_ix == 0)[0][0]\n",
    "         \n",
    "        \n",
    "            sort_timer = time.perf_counter()\n",
    "\n",
    "        \n",
    "\n",
    "        stats['generate_time'].append(generate_timer - start_timer)\n",
    "        stats['convert_time:'].append(convert_timer -  generate_timer)\n",
    "        stats['score_time'].append(score_timer-convert_timer)\n",
    "        stats['sort_time'].append(sort_timer-score_timer)\n",
    "        stats['total_time'].append(sort_timer-start_timer)\n",
    "        ranks.append(rank)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(rank),'\\n',{key: np.mean(value) for key,value in list(stats.items())[-100:]})\n",
    "            pass\n",
    "            \n",
    "        \n",
    "    stats =  {key: np.mean(value) for key,value in stats.items()}\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "    \n",
    "    stats['MR'] = np.mean(ranks)\n",
    "    stats['MRR'] = np.mean(1/(ranks))\n",
    "    stats['HITS1'] = np.count_nonzero(ranks <= 1)\n",
    "    stats['HITS3'] = np.count_nonzero(ranks <= 3)\n",
    "    stats['HITS10'] = np.count_nonzero(ranks <= 10)\n",
    "        \n",
    "    return  ranks, stats, embeddings_scores\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e6954-a0e6-4090-ac8d-8ec68ff16e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ranks, stats, table = evaluate_link_pred(lambda x : tf.keras.activations.sigmoid(model.predict(x)), testgraph, lambda x : word_vectors[x], testentities, max_triples=None)    \n",
    "#z, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e31b3-f2ea-4e14-aae0-a99137773ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = evaluate_link_pred(lambda x : tf.keras.activations.sigmoid(model.predict(x)), testgraph, lambda x : map_keyed_vectors(word_vectors,x), entities, max_triples=None)    \n",
    "#z, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e24a46-de07-4766-b3d9-6ea61a39d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_link_pred(lambda x : tf.keras.activations.sigmoid(model.predict(x)), g_val, lambda x : map_keyed_vectors(word_vectors,x), entities, max_triples=None)    \n",
    "#z, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fd86e-0f7e-43ad-84a1-c5c8e9f6cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    x**2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b6297-11ef-4e2a-8f91-f18d1ef1b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('square(12345)', number=10000000,globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6e8c7-6ca2-4879-ac59-c8e0d3a0093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit('(lambda x: x**2)(12345)', number=10000000,globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eba3fe-28d3-422e-a7b9-80ade6909b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d36ee-ea46-43ca-88d8-e393100e2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_link_pred(lambda x : tf.keras.activations.sigmoid(model.predict(x)), g_test, lambda x : word_vectors[x], entities, max_triples=None)    \n",
    "#z, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9ea63-6ee7-4cbe-8805-9e84703467a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_link_pred(lambda x : tf.keras.activations.sigmoid(model.predict(x)), g_train, lambda x : word_vectors[x], entities, max_triples=None)    \n",
    "#z, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f74b-8307-438b-936f-1f674e708afb",
   "metadata": {},
   "source": [
    "# Plot the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b1fb3-d088-4559-b050-5ac24734fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='both')\n",
    "#x_val , y_val= get_1_1_dataset(g_val,entities,lambda x : word_vectors[x])\n",
    "#x_test , y_test= get_1_1_dataset(g_test,entities,lambda x : word_vectors[x])\n",
    "\n",
    "x,y = X, Y\n",
    "#for x,y in [(X,Y),(x_val,y_val),(x_test,y_test)]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf0479-2467-40a5-b702-b8f960761751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "reduction_model = sklearn.manifold.TSNE(learning_rate='auto',init='pca').fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fde8b-19bf-42cd-ae51-3fa25b72032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(x,y,reduction_model=sklearn.manifold.TSNE(learning_rate='auto',init='pca').fit_transform,number_of_examples=10000):\n",
    "\"\"\"\n",
    "x = multi dim array (SAMPLES,EMBEDDING_DIM)\n",
    "y = one-dim array (SAMPLES,)\n",
    "\"\"\"\n",
    "    x,y = choose_many_multiple([x,y],number_of_examples)\n",
    "    x_reduced=reduction_model(x)\n",
    "\n",
    "    plot_data = np.concatenate((x_reduced,np.expand_dims(y,1)),1)\n",
    "\n",
    "    df = pd.DataFrame(plot_data,columns=list(range(x_tsne.shape[1]))+['label'])\n",
    "\n",
    "\n",
    "    fig = px.scatter(df, x=0, y=1, color=\"label\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9d692-da51-49cf-aae2-ae5a1f9ce590",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e24f8-7a75-4214-a9f5-1f59b720ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='both')\n",
    "\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275b55e-96cf-42f8-8ad9-c44d55b22bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='subject')\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387298af-5a0e-4dcc-9b56-2cae808552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_1_1_dataset(g_train,entities,lambda x : word_vectors[x],corrupt='object')\n",
    "plot_embeddings(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055a362",
   "metadata": {},
   "source": [
    "## Get walks into ram :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "\n",
    "f=gzip.open('walks/walk_file_0.txt.gz','rb')\n",
    "lines = [x.decode('utf-8') for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75e9c9-6aad-41e2-b1a2-aa2e1710a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std-env",
   "language": "python",
   "name": "std-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
